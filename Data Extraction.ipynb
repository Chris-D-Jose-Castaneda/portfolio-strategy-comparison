{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9237d4d3-a37f-43a5-ab81-573164e113a3",
   "metadata": {},
   "source": [
    "# Index‑Options Risk Snapshot with Polygon.io\n",
    "\n",
    "This notebook shows how to pull live **Greeks, implied volatility (IV), open‑interest, and price data** for **index options** (e.g., SPX, NDX, RUT, VIX) from the Polygon.io REST API and aggregate basic portfolio‑level risk measures.\n",
    "\n",
    "**What you’ll learn**\n",
    "\n",
    "1. How to query the **Option‑Chain Snapshot** endpoint for index underlyings (prefix `I:` per Polygon’s convention).  \n",
    "2. How to normalise the JSON into a Pandas DataFrame.  \n",
    "3. How to compute quick aggregate exposures—Δ, Γ, Θ, and ν (vega)—for an illustrative portfolio (here we assume **one contract per option**; multiply by your own position sizes).  \n",
    "\n",
    "> **Prerequisites**  \n",
    "> * A live Polygon.io key (`POLYGON_API_KEY` environment variable or paste directly).  \n",
    "> * `pandas`, `requests`, and (optionally) `pyarrow` for feather/Parquet export.\n",
    "\n",
    "> **Endpoint used**: `/v3/snapshot/options/{underlying}` (returns price, Greeks, IV, OI, quotes & trades) :contentReference[oaicite:0]{index=0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3df67fb5-1f0d-4528-a6ea-555e3163c661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31da47c3-f712-4cb0-8f62-db0ea554f29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from 2025-02-23 through 2025-07-18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching additional days: 100%|██████████████████████████████████████████████████████| 146/146 [00:49<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended 10000 new records; CSV now has 209000 rows.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "\n",
    "# ─── CONFIG ────────────────────────────────────────────────────────────────────\n",
    "API_KEY    = \"6f7zlNdhVy_4zN7ZZHSBLCwHv4cD6Ftw\"\n",
    "END_DATE   = datetime(2025, 7, 18)\n",
    "OUTPUT_CSV = \"SPX_options_reference_chain_20150101_20250718.csv\"\n",
    "\n",
    "# ─── HELPER (same as before) ───────────────────────────────────────────────────\n",
    "def fetch_contracts_as_of(date_str: str, api_key: str) -> list:\n",
    "    url = \"https://api.polygon.io/v3/reference/options/contracts\"\n",
    "    params = {\n",
    "        \"underlying_ticker\": \"SPX\",\n",
    "        \"as_of\": date_str,\n",
    "        \"limit\": 1000,\n",
    "        \"apiKey\": api_key\n",
    "    }\n",
    "    all_results = []\n",
    "    while url:\n",
    "        resp = requests.get(url, params=params).json()\n",
    "        all_results.extend(resp.get(\"results\", []))\n",
    "        url = resp.get(\"next_url\")\n",
    "        params = None\n",
    "    return all_results\n",
    "\n",
    "# ─── 1) LOAD EXISTING CSV & FIND RESUME DATE ───────────────────────────────────\n",
    "df_existing = pd.read_csv(OUTPUT_CSV, parse_dates=['snapshot_date'])\n",
    "last_date = df_existing['snapshot_date'].max()\n",
    "start_date = last_date + timedelta(days=1)\n",
    "print(f\"Resuming from {start_date.date()} through {END_DATE.date()}\")\n",
    "\n",
    "# ─── 2) LOOP & APPEND NEW DAYS ────────────────────────────────────────────────\n",
    "new_rows = []\n",
    "current = start_date\n",
    "pbar = tqdm(total=(END_DATE - start_date).days + 1, desc=\"Fetching additional days\")\n",
    "while current <= END_DATE:\n",
    "    date_str = current.strftime(\"%Y-%m-%d\")\n",
    "    try:\n",
    "        contracts = fetch_contracts_as_of(date_str, API_KEY)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ {date_str}: {e}\")\n",
    "        current += timedelta(days=1)\n",
    "        pbar.update(1)\n",
    "        continue\n",
    "\n",
    "    for c in contracts:\n",
    "        new_rows.append({\n",
    "            \"snapshot_date\":    date_str,\n",
    "            \"contract_symbol\":  c.get(\"ticker\"),\n",
    "            \"underlying_ticker\":c.get(\"underlying_ticker\"),\n",
    "            \"expiration_date\":  c.get(\"expiration_date\"),\n",
    "            \"strike_price\":     c.get(\"strike_price\"),\n",
    "            \"option_type\":      c.get(\"contract_type\"),\n",
    "            # …and any other fields you already included…\n",
    "        })\n",
    "\n",
    "    current += timedelta(days=1)\n",
    "    pbar.update(1)\n",
    "pbar.close()\n",
    "\n",
    "# ─── 3) CONCAT & WRITE BACK ────────────────────────────────────────────────────\n",
    "if new_rows:\n",
    "    df_new = pd.DataFrame(new_rows)\n",
    "    df_full = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "    df_full.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"Appended {len(df_new)} new records; CSV now has {len(df_full)} rows.\")\n",
    "else:\n",
    "    print(\"Nothing new to append—you're fully up to date!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8974bf44-0635-44e5-bce9-96258e66b724",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching metadata:   0%|                                                         | 18/35902 [01:18<37:53:13,  3.80s/it]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "\n",
    "# ─── CONFIG ───────────────────────────────────────────────────────────────\n",
    "API_KEY   = \"6f7zlNdhVy_4zN7ZZHSBLCwHv4cD6Ftw\"\n",
    "INPUT_CSV = \"SPX_options_reference_chain_20150101_20250718.csv\"\n",
    "OUTPUT_CSV= \"SPX_options_reference_chain_20150101_20250718_populated.csv\"\n",
    "\n",
    "# ─── 1) LOAD CSV ──────────────────────────────────────────────────────────\n",
    "df = pd.read_csv(\n",
    "    INPUT_CSV,\n",
    "    parse_dates=['snapshot_date', 'expiration_date'],\n",
    "    dtype={'contract_symbol': str}\n",
    ")\n",
    "\n",
    "# ─── 2) SET UP A SESSION WITH RETRIES ─────────────────────────────────────\n",
    "session = requests.Session()\n",
    "retries = Retry(total=5, backoff_factor=0.5,\n",
    "                status_forcelist=[429, 500, 502, 503, 504])\n",
    "session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "\n",
    "# ─── 3) PREPARE METADATA DICT ─────────────────────────────────────────────\n",
    "# Ensure these columns exist (so merge will fill them)\n",
    "for col in ['exercise_style','multiplier','expiration_type',\n",
    "            'listing_date','updated_at','active']:\n",
    "    df[col] = pd.NA\n",
    "\n",
    "symbols = df['contract_symbol'].dropna().unique()\n",
    "metadata = {}\n",
    "\n",
    "# ─── 4) FETCH PER SYMBOL WITH ERROR HANDLING ──────────────────────────────\n",
    "for sym in tqdm(symbols, desc=\"Fetching metadata\"):\n",
    "    url = f\"https://api.polygon.io/v3/reference/options/contracts/{sym}\"\n",
    "    try:\n",
    "        resp = session.get(url, params={\"apiKey\": API_KEY}, timeout=10).json()\n",
    "        c = resp.get(\"results\", {}) or {}\n",
    "        metadata[sym] = {\n",
    "            'exercise_style':   c.get('exercise_style'),\n",
    "            'multiplier':       c.get('multiplier'),\n",
    "            'expiration_type':  c.get('expiration_type'),\n",
    "            'listing_date':     c.get('listing_date'),\n",
    "            'updated_at':       c.get('updated_at'),\n",
    "            'active':           c.get('active')\n",
    "        }\n",
    "    except Exception as e:\n",
    "        # On error, fill with NaNs but continue\n",
    "        metadata[sym] = dict.fromkeys(\n",
    "            ['exercise_style','multiplier','expiration_type',\n",
    "             'listing_date','updated_at','active'],\n",
    "            pd.NA\n",
    "        )\n",
    "\n",
    "# ─── 5) MERGE METADATA BACK ────────────────────────────────────────────────\n",
    "meta_df = pd.DataFrame.from_dict(\n",
    "    metadata, orient='index'\n",
    ").rename_axis('contract_symbol').reset_index()\n",
    "\n",
    "df = df.drop(\n",
    "    columns=['exercise_style','multiplier','expiration_type',\n",
    "             'listing_date','updated_at','active'],\n",
    "    errors='ignore'\n",
    ").merge(meta_df, on='contract_symbol', how='left')\n",
    "\n",
    "# ─── 6) SAVE FINAL CSV ─────────────────────────────────────────────────────\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"✅ Populated metadata for all symbols and saved to {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062fe263-a6f4-4664-8e16-01c0a0ae04cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
